{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear soft margin svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.2697e+01  5.1817e+02  8e+03  3e+00  5e+02\n",
      " 1:  2.0900e+02 -2.2996e+02  5e+02  1e-01  2e+01\n",
      " 2:  1.1640e+02  1.9175e+01  1e+02  1e-02  2e+00\n",
      " 3:  5.8976e+01  4.1479e+01  2e+01  2e-03  4e-01\n",
      " 4:  5.0881e+01  4.7086e+01  4e+00  4e-04  7e-02\n",
      " 5:  4.9575e+01  4.8181e+01  1e+00  1e-04  2e-02\n",
      " 6:  4.9191e+01  4.8499e+01  7e-01  5e-05  8e-03\n",
      " 7:  4.8992e+01  4.8667e+01  3e-01  2e-05  3e-03\n",
      " 8:  4.8875e+01  4.8767e+01  1e-01  5e-06  8e-04\n",
      " 9:  4.8830e+01  4.8807e+01  2e-02  7e-07  1e-04\n",
      "10:  4.8820e+01  4.8816e+01  4e-03  1e-07  2e-05\n",
      "11:  4.8818e+01  4.8818e+01  4e-04  1e-08  2e-06\n",
      "12:  4.8818e+01  4.8818e+01  4e-06  1e-10  2e-08\n",
      "Optimal solution found.\n",
      "Custom Linear SVM Results:\n",
      "Test Accuracy: 0.8678\n",
      "Train Accuracy: 0.7944\n",
      "Training Time: 0.6436 seconds\n",
      "Number of Support Vectors: 5\n",
      "Weight vector (w): [ 1.13674498  1.05522057 -0.16442498  0.15522254]\n",
      "Bias term (b): 0.013285141925986578\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "def linear_soft_margin_svm(X, y, C):\n",
    "    \"\"\"\n",
    "    Solves the soft-margin SVM optimization problem using quadratic programming.\n",
    "\n",
    "    Args:\n",
    "        X: Input data (N x d), where N is the number of samples and d is the feature dimension.\n",
    "        y: Labels (N x 1), where each label is either +1 or -1.\n",
    "        C: Regularization parameter.\n",
    "\n",
    "    Returns:\n",
    "        w: Weight vector (d x 1).\n",
    "        b: Bias term.\n",
    "    \"\"\"\n",
    "    N, d = X.shape\n",
    "\n",
    "    # Construct the matrices for the QP solver\n",
    "\n",
    "    # P: Block diagonal matrix (d + 1 + N) x (d + 1 + N)\n",
    "    P = np.zeros((d + 1 + N, d + 1 + N))\n",
    "    P[:d, :d] = np.eye(d)  # Penalize w but not b or xi\n",
    "    P = matrix(P)\n",
    "\n",
    "    # q: Linear term (d + 1 + N)x1\n",
    "    q = np.zeros(d + 1 + N)\n",
    "    q[d + 1:] = C  # Penalize xi with factor C\n",
    "    q = matrix(q)\n",
    "\n",
    "    # G: Inequality constraint coefficients\n",
    "    G = np.zeros((2 * N, d + 1 + N))\n",
    "    \n",
    "    # y_i(w^T x_i + b) + xi >= 1\n",
    "    for i in range(N):\n",
    "        G[i, :d] = -y[i] * X[i]\n",
    "        G[i, d] = -y[i]  # Bias term\n",
    "        G[i, d + 1 + i] = -1  # Slack variable xi\n",
    "\n",
    "    # xi >= 0\n",
    "    for i in range(N):\n",
    "        G[N + i, d + 1 + i] = -1\n",
    "\n",
    "    G = matrix(G)\n",
    "\n",
    "    # h: Right-hand side for inequality constraints\n",
    "    h = np.zeros(2 * N)\n",
    "    h[:N] = -1  # y_i(w^T x_i + b) + xi >= 1\n",
    "    h = matrix(h)\n",
    "\n",
    "    # A and b: Equality constraints (none for soft-margin SVM)\n",
    "    A = None\n",
    "    b = None\n",
    "\n",
    "    # Solve the QP problem\n",
    "    sol = solvers.qp(P, q, G, h, A, b)\n",
    "    solution = np.array(sol['x']).flatten()\n",
    "\n",
    "    # Extract w and b from the solution\n",
    "    w = solution[:d]\n",
    "    b = solution[d]\n",
    "\n",
    "    return w, b\n",
    "\n",
    "def predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Predicts the labels for the input data using the learned SVM model.\n",
    "\n",
    "    Args:\n",
    "        X: Input data (N x d), where N is the number of samples and d is the feature dimension.\n",
    "        w: Weight vector (d x 1).\n",
    "        b: Bias term.\n",
    "\n",
    "    Returns:\n",
    "        Predicted labels (N x 1).\n",
    "    \"\"\"\n",
    "    return np.sign(np.dot(X, w) + b)\n",
    "\n",
    "def calculate_accuracy(X_test, y_test, w, b):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the SVM model on the test set.\n",
    "\n",
    "    Args:\n",
    "        X_test: Test data (N x d).\n",
    "        y_test: True labels for the test data (N x 1).\n",
    "        w: Weight vector.\n",
    "        b: Bias term.\n",
    "\n",
    "    Returns:\n",
    "        Accuracy of the model.\n",
    "    \"\"\"\n",
    "    predictions = predict(X_test, w, b)\n",
    "    accuracy = np.mean(predictions == y_test)\n",
    "    return accuracy\n",
    "\n",
    "def calculate_support_vectors(X, y, w, b, C):\n",
    "    \"\"\"\n",
    "    Calculate the number of support vectors by checking the decision function.\n",
    "\n",
    "    Args:\n",
    "        X: Data points (N x d).\n",
    "        y: Labels (N x 1).\n",
    "        w: Weight vector.\n",
    "        b: Bias term.\n",
    "        C: Regularization parameter.\n",
    "\n",
    "    Returns:\n",
    "        Number of support vectors.\n",
    "    \"\"\"\n",
    "    decision_values = y * (np.dot(X, w) + b)\n",
    "    support_vectors = np.sum(np.abs(decision_values - 1) <= 1e-5)\n",
    "\n",
    "    return support_vectors\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    X = pd.read_excel(\"../coffeeDataSynthesized.xlsx\", \"dataset\")\n",
    "    y = np.where(X[\"type\"] == \"robusta\", 0, 1)  # Robusta -> 0, Arabica -> 1\n",
    "    y = pd.Series(y)\n",
    "\n",
    "    # Select relevant features\n",
    "    X = X[['width', 'height', 'depth', 'weight']]\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert y_train and y_test to -1 and +1\n",
    "    y_train = np.where(y_train == 0, -1, 1)\n",
    "    y_test = np.where(y_test == 0, -1, 1)\n",
    "\n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    w, b = linear_soft_margin_svm(X_train_scaled, y_train, C=0.1)\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    # Predict and evaluate custom SVM\n",
    "    y_pred_custom = predict(X_test_scaled, w, b)\n",
    "    y_train_pred_custom = predict(X_train_scaled, w, b)\n",
    "    custom_accuracy = np.mean(y_pred_custom == y_test)\n",
    "    custom_accuracy_train = np.mean(y_train_pred_custom == y_train)\n",
    "\n",
    "    # Output results\n",
    "    print(\"Custom Linear SVM Results:\")\n",
    "    print(f\"Test Accuracy: {custom_accuracy:.4f}\")\n",
    "    print(f\"Train Accuracy: {custom_accuracy_train:.4f}\")\n",
    "    print(f\"Training Time: {training_time:.4f} seconds\")\n",
    "\n",
    "    # Calculate number of support vectors\n",
    "    num_support_vectors = calculate_support_vectors(X_train_scaled, y_train, w, b, C=0.1)\n",
    "    print(f\"Number of Support Vectors: {num_support_vectors}\")\n",
    "\n",
    "    print(\"Weight vector (w):\", w)\n",
    "    print(\"Bias term (b):\", b)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[100  20]\n",
      " [ 12 110]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred_custom)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.7482e+01  4.0268e+02  6e+03  3e+00  4e+02\n",
      " 1:  1.6161e+02 -1.8768e+02  4e+02  1e-01  2e+01\n",
      " 2:  9.1942e+01  1.2225e+01  8e+01  1e-02  2e+00\n",
      " 3:  4.6940e+01  3.0891e+01  2e+01  3e-03  4e-01\n",
      " 4:  3.9104e+01  3.5712e+01  3e+00  5e-04  8e-02\n",
      " 5:  3.7916e+01  3.6687e+01  1e+00  1e-04  2e-02\n",
      " 6:  3.7544e+01  3.6985e+01  6e-01  5e-05  7e-03\n",
      " 7:  3.7348e+01  3.7143e+01  2e-01  1e-05  2e-03\n",
      " 8:  3.7265e+01  3.7214e+01  5e-02  2e-06  4e-04\n",
      " 9:  3.7244e+01  3.7232e+01  1e-02  5e-07  7e-05\n",
      "10:  3.7238e+01  3.7238e+01  4e-04  1e-08  2e-06\n",
      "11:  3.7238e+01  3.7238e+01  5e-06  1e-10  2e-08\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.7857e+01  4.1021e+02  6e+03  3e+00  4e+02\n",
      " 1:  1.6589e+02 -1.8498e+02  4e+02  1e-01  2e+01\n",
      " 2:  9.2558e+01  1.5317e+01  8e+01  1e-02  2e+00\n",
      " 3:  4.5537e+01  3.3485e+01  1e+01  2e-03  3e-01\n",
      " 4:  4.0495e+01  3.7535e+01  3e+00  4e-04  6e-02\n",
      " 5:  3.9348e+01  3.8596e+01  8e-01  6e-05  9e-03\n",
      " 6:  3.9014e+01  3.8890e+01  1e-01  8e-06  1e-03\n",
      " 7:  3.8961e+01  3.8936e+01  3e-02  1e-06  2e-04\n",
      " 8:  3.8949e+01  3.8947e+01  2e-03  9e-08  1e-05\n",
      " 9:  3.8948e+01  3.8948e+01  1e-04  4e-16  3e-14\n",
      "10:  3.8948e+01  3.8948e+01  5e-06  4e-16  1e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.8923e+01  4.2004e+02  6e+03  3e+00  4e+02\n",
      " 1:  1.6688e+02 -1.5935e+02  3e+02  1e-01  1e+01\n",
      " 2:  8.9551e+01  2.0765e+01  7e+01  1e-02  1e+00\n",
      " 3:  4.6668e+01  3.5873e+01  1e+01  2e-03  2e-01\n",
      " 4:  4.2205e+01  3.9626e+01  3e+00  3e-04  4e-02\n",
      " 5:  4.1393e+01  4.0367e+01  1e+00  1e-04  1e-02\n",
      " 6:  4.1024e+01  4.0696e+01  3e-01  3e-05  4e-03\n",
      " 7:  4.0901e+01  4.0806e+01  1e-01  6e-06  9e-04\n",
      " 8:  4.0862e+01  4.0842e+01  2e-02  1e-06  2e-04\n",
      " 9:  4.0853e+01  4.0851e+01  2e-03  4e-08  6e-06\n",
      "10:  4.0852e+01  4.0852e+01  3e-05  6e-10  9e-08\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.7953e+01  4.1260e+02  6e+03  3e+00  4e+02\n",
      " 1:  1.6757e+02 -1.8075e+02  4e+02  1e-01  2e+01\n",
      " 2:  9.1640e+01  1.6515e+01  8e+01  1e-02  2e+00\n",
      " 3:  4.5983e+01  3.3080e+01  1e+01  2e-03  3e-01\n",
      " 4:  4.0095e+01  3.7357e+01  3e+00  3e-04  5e-02\n",
      " 5:  3.9058e+01  3.8259e+01  8e-01  8e-05  1e-02\n",
      " 6:  3.8819e+01  3.8459e+01  4e-01  3e-05  4e-03\n",
      " 7:  3.8686e+01  3.8571e+01  1e-01  7e-06  1e-03\n",
      " 8:  3.8644e+01  3.8606e+01  4e-02  6e-07  9e-05\n",
      " 9:  3.8627e+01  3.8622e+01  5e-03  7e-08  1e-05\n",
      "10:  3.8625e+01  3.8625e+01  9e-05  1e-09  1e-07\n",
      "11:  3.8625e+01  3.8625e+01  9e-07  1e-11  1e-09\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.8547e+01  4.1695e+02  6e+03  3e+00  4e+02\n",
      " 1:  1.6669e+02 -1.7331e+02  4e+02  1e-01  2e+01\n",
      " 2:  9.1079e+01  1.9157e+01  7e+01  1e-02  2e+00\n",
      " 3:  4.5520e+01  3.5299e+01  1e+01  2e-03  2e-01\n",
      " 4:  4.1509e+01  3.9129e+01  2e+00  3e-04  4e-02\n",
      " 5:  4.0660e+01  3.9968e+01  7e-01  6e-05  9e-03\n",
      " 6:  4.0449e+01  4.0168e+01  3e-01  2e-05  3e-03\n",
      " 7:  4.0349e+01  4.0261e+01  9e-02  5e-06  8e-04\n",
      " 8:  4.0311e+01  4.0297e+01  1e-02  5e-07  7e-05\n",
      " 9:  4.0304e+01  4.0304e+01  9e-04  2e-08  3e-06\n",
      "10:  4.0304e+01  4.0304e+01  1e-05  3e-10  4e-08\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.1805e+02  1.9593e+03  9e+03  4e+00  4e+01\n",
      " 1:  1.0552e+03 -2.4819e+02  2e+03  5e-01  5e+00\n",
      " 2:  5.6051e+02  2.1146e+02  4e+02  1e-01  1e+00\n",
      " 3:  3.9545e+02  3.1774e+02  9e+01  2e-02  2e-01\n",
      " 4:  3.7310e+02  3.4250e+02  3e+01  6e-03  6e-02\n",
      " 5:  3.6489e+02  3.5135e+02  1e+01  2e-03  2e-02\n",
      " 6:  3.6063e+02  3.5568e+02  5e+00  6e-04  6e-03\n",
      " 7:  3.5944e+02  3.5683e+02  3e+00  3e-04  3e-03\n",
      " 8:  3.5861e+02  3.5763e+02  1e+00  8e-05  8e-04\n",
      " 9:  3.5822e+02  3.5801e+02  2e-01  7e-06  7e-05\n",
      "10:  3.5812e+02  3.5811e+02  1e-02  4e-07  4e-06\n",
      "11:  3.5811e+02  3.5811e+02  1e-04  4e-09  4e-08\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.1104e+02  2.0326e+03  1e+04  4e+00  4e+01\n",
      " 1:  1.1216e+03 -2.7418e+02  2e+03  5e-01  5e+00\n",
      " 2:  5.6551e+02  2.4248e+02  4e+02  9e-02  9e-01\n",
      " 3:  4.1172e+02  3.3997e+02  8e+01  2e-02  2e-01\n",
      " 4:  3.9143e+02  3.6275e+02  3e+01  5e-03  5e-02\n",
      " 5:  3.8438e+02  3.6979e+02  2e+01  2e-03  2e-02\n",
      " 6:  3.7851e+02  3.7563e+02  3e+00  2e-04  2e-03\n",
      " 7:  3.7735e+02  3.7671e+02  7e-01  4e-05  4e-04\n",
      " 8:  3.7723e+02  3.7681e+02  4e-01  2e-05  2e-04\n",
      " 9:  3.7707e+02  3.7696e+02  1e-01  4e-06  4e-05\n",
      "10:  3.7703e+02  3.7700e+02  3e-02  9e-07  9e-06\n",
      "11:  3.7701e+02  3.7701e+02  3e-03  4e-08  4e-07\n",
      "12:  3.7701e+02  3.7701e+02  3e-05  4e-10  4e-09\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.9157e+02  2.0358e+03  9e+03  4e+00  4e+01\n",
      " 1:  1.1380e+03 -2.2386e+02  2e+03  5e-01  5e+00\n",
      " 2:  5.7493e+02  2.6890e+02  4e+02  8e-02  8e-01\n",
      " 3:  4.2621e+02  3.6267e+02  7e+01  2e-02  1e-01\n",
      " 4:  4.0932e+02  3.8477e+02  3e+01  4e-03  4e-02\n",
      " 5:  4.0458e+02  3.9017e+02  2e+01  2e-03  2e-02\n",
      " 6:  4.0113e+02  3.9382e+02  8e+00  9e-04  9e-03\n",
      " 7:  3.9881e+02  3.9615e+02  3e+00  3e-04  3e-03\n",
      " 8:  3.9793e+02  3.9703e+02  9e-01  7e-05  6e-04\n",
      " 9:  3.9761e+02  3.9732e+02  3e-01  9e-06  9e-05\n",
      "10:  3.9752e+02  3.9742e+02  1e-01  4e-16  9e-14\n",
      "11:  3.9747e+02  3.9746e+02  1e-02  4e-16  5e-13\n",
      "12:  3.9747e+02  3.9747e+02  2e-04  4e-16  1e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.1024e+02  2.0584e+03  1e+04  4e+00  4e+01\n",
      " 1:  1.1387e+03 -2.9002e+02  2e+03  5e-01  5e+00\n",
      " 2:  5.7360e+02  2.3349e+02  4e+02  9e-02  1e+00\n",
      " 3:  4.0793e+02  3.3365e+02  8e+01  2e-02  2e-01\n",
      " 4:  3.8562e+02  3.5908e+02  3e+01  5e-03  5e-02\n",
      " 5:  3.7981e+02  3.6561e+02  2e+01  2e-03  2e-02\n",
      " 6:  3.7527e+02  3.6991e+02  6e+00  5e-04  6e-03\n",
      " 7:  3.7391e+02  3.7118e+02  3e+00  2e-04  2e-03\n",
      " 8:  3.7304e+02  3.7200e+02  1e+00  8e-05  8e-04\n",
      " 9:  3.7264e+02  3.7237e+02  3e-01  9e-06  9e-05\n",
      "10:  3.7253e+02  3.7248e+02  5e-02  1e-07  1e-06\n",
      "11:  3.7251e+02  3.7250e+02  1e-02  2e-08  2e-07\n",
      "12:  3.7250e+02  3.7250e+02  9e-04  1e-09  1e-08\n",
      "13:  3.7250e+02  3.7250e+02  9e-06  1e-11  1e-10\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.9929e+02  2.0386e+03  9e+03  4e+00  4e+01\n",
      " 1:  1.1350e+03 -2.4497e+02  2e+03  5e-01  5e+00\n",
      " 2:  5.3959e+02  2.8143e+02  3e+02  6e-02  6e-01\n",
      " 3:  4.1755e+02  3.6183e+02  6e+01  1e-02  1e-01\n",
      " 4:  4.0491e+02  3.7736e+02  3e+01  5e-03  5e-02\n",
      " 5:  3.9895e+02  3.8421e+02  2e+01  2e-03  2e-02\n",
      " 6:  3.9501e+02  3.8830e+02  7e+00  9e-04  9e-03\n",
      " 7:  3.9334e+02  3.9008e+02  3e+00  4e-04  4e-03\n",
      " 8:  3.9228e+02  3.9115e+02  1e+00  1e-04  1e-03\n",
      " 9:  3.9189e+02  3.9155e+02  4e-01  2e-05  2e-04\n",
      "10:  3.9175e+02  3.9169e+02  7e-02  3e-06  3e-05\n",
      "11:  3.9172e+02  3.9172e+02  9e-04  4e-08  4e-07\n",
      "12:  3.9172e+02  3.9172e+02  9e-06  4e-10  4e-09\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.2337e+04  4.4540e+04  2e+05  2e+01  1e+01\n",
      " 1:  1.3923e+04 -1.2542e+03  3e+04  2e+00  1e+00\n",
      " 2:  5.5184e+03  2.1921e+03  5e+03  3e-01  2e-01\n",
      " 3:  4.0028e+03  2.9494e+03  1e+03  8e-02  4e-02\n",
      " 4:  3.8402e+03  3.2386e+03  8e+02  3e-02  2e-02\n",
      " 5:  3.7055e+03  3.4080e+03  4e+02  1e-02  7e-03\n",
      " 6:  3.6368e+03  3.4889e+03  2e+02  5e-03  3e-03\n",
      " 7:  3.6049e+03  3.5217e+03  1e+02  2e-03  1e-03\n",
      " 8:  3.5786e+03  3.5477e+03  3e+01  5e-04  3e-04\n",
      " 9:  3.5675e+03  3.5583e+03  1e+01  1e-04  8e-05\n",
      "10:  3.5643e+03  3.5612e+03  3e+00  2e-05  1e-05\n",
      "11:  3.5637e+03  3.5618e+03  2e+00  6e-06  3e-06\n",
      "12:  3.5629e+03  3.5626e+03  2e-01  6e-07  3e-07\n",
      "13:  3.5627e+03  3.5627e+03  4e-03  8e-09  5e-09\n",
      "14:  3.5627e+03  3.5627e+03  4e-05  8e-11  5e-11\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.1951e+04  4.2860e+04  1e+05  2e+01  1e+01\n",
      " 1:  1.4103e+04 -4.8222e+02  2e+04  2e+00  9e-01\n",
      " 2:  5.3479e+03  2.4170e+03  4e+03  2e-01  1e-01\n",
      " 3:  4.1785e+03  3.2044e+03  1e+03  7e-02  3e-02\n",
      " 4:  4.0556e+03  3.4419e+03  8e+02  3e-02  2e-02\n",
      " 5:  3.9442e+03  3.5706e+03  5e+02  2e-02  8e-03\n",
      " 6:  3.8745e+03  3.6416e+03  3e+02  8e-03  4e-03\n",
      " 7:  3.8310e+03  3.6826e+03  2e+02  5e-03  3e-03\n",
      " 8:  3.7942e+03  3.7174e+03  9e+01  2e-03  1e-03\n",
      " 9:  3.7705e+03  3.7400e+03  3e+01  7e-04  4e-04\n",
      "10:  3.7567e+03  3.7529e+03  4e+00  7e-05  4e-05\n",
      "11:  3.7551e+03  3.7544e+03  7e-01  1e-05  5e-06\n",
      "12:  3.7549e+03  3.7546e+03  3e-01  3e-06  2e-06\n",
      "13:  3.7548e+03  3.7547e+03  8e-02  5e-07  3e-07\n",
      "14:  3.7547e+03  3.7547e+03  6e-03  4e-08  2e-08\n",
      "15:  3.7547e+03  3.7547e+03  6e-05  4e-10  2e-10\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.0880e+04  3.8245e+04  1e+05  2e+01  1e+01\n",
      " 1:  1.2849e+04  6.0376e+02  2e+04  1e+00  6e-01\n",
      " 2:  4.7951e+03  2.7116e+03  3e+03  2e-01  8e-02\n",
      " 3:  4.3426e+03  3.4652e+03  1e+03  6e-02  3e-02\n",
      " 4:  4.2055e+03  3.6776e+03  7e+02  3e-02  1e-02\n",
      " 5:  4.0955e+03  3.8127e+03  4e+02  1e-02  6e-03\n",
      " 6:  4.0328e+03  3.8844e+03  2e+02  6e-03  3e-03\n",
      " 7:  3.9951e+03  3.9259e+03  8e+01  2e-03  1e-03\n",
      " 8:  3.9741e+03  3.9477e+03  3e+01  5e-04  2e-04\n",
      " 9:  3.9658e+03  3.9559e+03  1e+01  2e-04  7e-05\n",
      "10:  3.9632e+03  3.9584e+03  5e+00  6e-05  3e-05\n",
      "11:  3.9614e+03  3.9602e+03  1e+00  1e-05  6e-06\n",
      "12:  3.9609e+03  3.9607e+03  2e-01  7e-07  3e-07\n",
      "13:  3.9608e+03  3.9608e+03  1e-02  5e-08  2e-08\n",
      "14:  3.9608e+03  3.9608e+03  1e-04  5e-10  3e-10\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.1952e+04  4.1352e+04  1e+05  2e+01  1e+01\n",
      " 1:  1.3372e+04 -2.1341e+02  2e+04  2e+00  8e-01\n",
      " 2:  5.0721e+03  2.4448e+03  4e+03  2e-01  1e-01\n",
      " 3:  4.0842e+03  3.2172e+03  1e+03  5e-02  3e-02\n",
      " 4:  3.9827e+03  3.3952e+03  8e+02  3e-02  2e-02\n",
      " 5:  3.8309e+03  3.5663e+03  3e+02  1e-02  6e-03\n",
      " 6:  3.7906e+03  3.6272e+03  2e+02  4e-03  2e-03\n",
      " 7:  3.7476e+03  3.6704e+03  9e+01  2e-03  8e-04\n",
      " 8:  3.7273e+03  3.6900e+03  4e+01  7e-04  4e-04\n",
      " 9:  3.7164e+03  3.7001e+03  2e+01  2e-04  1e-04\n",
      "10:  3.7102e+03  3.7061e+03  4e+00  3e-05  2e-05\n",
      "11:  3.7083e+03  3.7079e+03  4e-01  3e-06  1e-06\n",
      "12:  3.7081e+03  3.7081e+03  8e-03  1e-08  7e-09\n",
      "13:  3.7081e+03  3.7081e+03  8e-05  1e-10  7e-11\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.1349e+04  4.0759e+04  1e+05  2e+01  1e+01\n",
      " 1:  1.4746e+04  5.3900e+02  2e+04  1e+00  6e-01\n",
      " 2:  4.9167e+03  2.6755e+03  3e+03  1e-01  7e-02\n",
      " 3:  4.3530e+03  3.4196e+03  1e+03  5e-02  2e-02\n",
      " 4:  4.1313e+03  3.6809e+03  5e+02  2e-02  8e-03\n",
      " 5:  4.0447e+03  3.7722e+03  3e+02  9e-03  4e-03\n",
      " 6:  3.9754e+03  3.8374e+03  2e+02  4e-03  2e-03\n",
      " 7:  3.9378e+03  3.8729e+03  7e+01  2e-03  7e-04\n",
      " 8:  3.9187e+03  3.8908e+03  3e+01  6e-04  3e-04\n",
      " 9:  3.9110e+03  3.8978e+03  1e+01  2e-04  1e-04\n",
      "10:  3.9059e+03  3.9026e+03  4e+00  4e-05  2e-05\n",
      "11:  3.9044e+03  3.9040e+03  5e-01  4e-06  2e-06\n",
      "12:  3.9042e+03  3.9042e+03  5e-03  5e-08  2e-08\n",
      "13:  3.9042e+03  3.9042e+03  5e-05  5e-10  2e-10\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.4389e+06  3.2495e+06  1e+07  2e+02  9e+00\n",
      " 1:  6.9932e+05 -1.9971e+05  2e+06  2e+01  8e-01\n",
      " 2:  1.7062e+05  1.5115e+04  2e+05  2e+00  7e-02\n",
      " 3:  4.9475e+04  2.3075e+04  4e+04  3e-01  1e-02\n",
      " 4:  4.5002e+04  2.7491e+04  2e+04  1e-01  6e-03\n",
      " 5:  4.1375e+04  3.0692e+04  1e+04  6e-02  3e-03\n",
      " 6:  3.8964e+04  3.2636e+04  8e+03  3e-02  2e-03\n",
      " 7:  3.8002e+04  3.3513e+04  6e+03  2e-02  9e-04\n",
      " 8:  3.7103e+04  3.4308e+04  3e+03  1e-02  5e-04\n",
      " 9:  3.6341e+04  3.4983e+04  2e+03  4e-03  2e-04\n",
      "10:  3.6049e+04  3.5233e+04  9e+02  2e-03  1e-04\n",
      "11:  3.5811e+04  3.5431e+04  4e+02  9e-04  4e-05\n",
      "12:  3.5661e+04  3.5561e+04  1e+02  9e-05  4e-06\n",
      "13:  3.5622e+04  3.5597e+04  3e+01  2e-05  1e-06\n",
      "14:  3.5618e+04  3.5600e+04  2e+01  1e-05  5e-07\n",
      "15:  3.5617e+04  3.5601e+04  2e+01  7e-06  3e-07\n",
      "16:  3.5612e+04  3.5606e+04  5e+00  2e-06  1e-07\n",
      "17:  3.5609e+04  3.5609e+04  2e-01  1e-08  6e-10\n",
      "18:  3.5609e+04  3.5609e+04  2e-03  1e-10  6e-12\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.4034e+06  3.0920e+06  1e+07  2e+02  9e+00\n",
      " 1:  7.2636e+05 -1.4809e+05  2e+06  1e+01  6e-01\n",
      " 2:  1.4099e+05  1.9142e+04  2e+05  1e+00  5e-02\n",
      " 3:  4.9054e+04  2.5143e+04  3e+04  2e-01  9e-03\n",
      " 4:  4.5346e+04  2.9987e+04  2e+04  1e-01  5e-03\n",
      " 5:  4.2365e+04  3.2969e+04  1e+04  5e-02  2e-03\n",
      " 6:  4.0132e+04  3.5017e+04  6e+03  2e-02  1e-03\n",
      " 7:  3.9352e+04  3.5866e+04  4e+03  1e-02  6e-04\n",
      " 8:  3.8697e+04  3.6493e+04  3e+03  8e-03  3e-04\n",
      " 9:  3.8323e+04  3.6828e+04  2e+03  4e-03  2e-04\n",
      "10:  3.7999e+04  3.7114e+04  1e+03  2e-03  1e-04\n",
      "11:  3.7726e+04  3.7358e+04  4e+02  8e-04  3e-05\n",
      "12:  3.7555e+04  3.7510e+04  5e+01  7e-05  3e-06\n",
      "13:  3.7537e+04  3.7527e+04  1e+01  1e-05  6e-07\n",
      "14:  3.7533e+04  3.7531e+04  2e+00  2e-06  8e-08\n",
      "15:  3.7532e+04  3.7531e+04  1e+00  8e-07  3e-08\n",
      "16:  3.7532e+04  3.7532e+04  1e-01  7e-08  3e-09\n",
      "17:  3.7532e+04  3.7532e+04  1e-03  7e-10  3e-11\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.3051e+06  2.6594e+06  9e+06  2e+02  7e+00\n",
      " 1:  6.1060e+05 -8.5954e+04  1e+06  1e+01  5e-01\n",
      " 2:  8.6996e+04  2.2603e+04  1e+05  6e-01  2e-02\n",
      " 3:  4.5811e+04  2.8948e+04  2e+04  1e-01  5e-03\n",
      " 4:  4.4555e+04  3.3089e+04  2e+04  7e-02  3e-03\n",
      " 5:  4.2490e+04  3.5909e+04  9e+03  4e-02  1e-03\n",
      " 6:  4.1889e+04  3.6892e+04  6e+03  2e-02  9e-04\n",
      " 7:  4.1025e+04  3.7980e+04  4e+03  1e-02  5e-04\n",
      " 8:  4.0484e+04  3.8628e+04  2e+03  7e-03  3e-04\n",
      " 9:  4.0009e+04  3.9171e+04  1e+03  2e-03  9e-05\n",
      "10:  3.9825e+04  3.9362e+04  5e+02  9e-04  4e-05\n",
      "11:  3.9699e+04  3.9492e+04  2e+02  3e-04  1e-05\n",
      "12:  3.9636e+04  3.9553e+04  9e+01  1e-04  4e-06\n",
      "13:  3.9612e+04  3.9576e+04  4e+01  4e-05  1e-06\n",
      "14:  3.9597e+04  3.9591e+04  6e+00  4e-06  1e-07\n",
      "15:  3.9595e+04  3.9593e+04  3e+00  6e-07  2e-08\n",
      "16:  3.9594e+04  3.9594e+04  5e-02  1e-08  4e-10\n",
      "17:  3.9594e+04  3.9594e+04  5e-04  1e-10  4e-12\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.4044e+06  2.9519e+06  1e+07  2e+02  8e+00\n",
      " 1:  6.7853e+05 -1.2759e+05  2e+06  1e+01  6e-01\n",
      " 2:  1.2060e+05  2.0121e+04  1e+05  8e-01  3e-02\n",
      " 3:  4.8876e+04  2.5474e+04  3e+04  2e-01  7e-03\n",
      " 4:  4.3746e+04  3.0685e+04  2e+04  8e-02  3e-03\n",
      " 5:  4.1512e+04  3.3016e+04  1e+04  4e-02  2e-03\n",
      " 6:  4.0076e+04  3.4308e+04  7e+03  3e-02  1e-03\n",
      " 7:  3.8544e+04  3.5653e+04  3e+03  1e-02  4e-04\n",
      " 8:  3.7909e+04  3.6295e+04  2e+03  4e-03  2e-04\n",
      " 9:  3.7491e+04  3.6680e+04  9e+02  2e-03  8e-05\n",
      "10:  3.7274e+04  3.6879e+04  4e+02  6e-04  3e-05\n",
      "11:  3.7171e+04  3.6965e+04  2e+02  2e-04  1e-05\n",
      "12:  3.7122e+04  3.7009e+04  1e+02  8e-05  3e-06\n",
      "13:  3.7075e+04  3.7053e+04  2e+01  2e-06  7e-08\n",
      "14:  3.7065e+04  3.7062e+04  2e+00  2e-07  8e-09\n",
      "15:  3.7065e+04  3.7063e+04  2e+00  2e-07  7e-09\n",
      "16:  3.7064e+04  3.7063e+04  2e+00  6e-08  3e-09\n",
      "17:  3.7064e+04  3.7063e+04  9e-01  3e-08  1e-09\n",
      "18:  3.7064e+04  3.7063e+04  5e-01  3e-09  1e-10\n",
      "19:  3.7064e+04  3.7064e+04  6e-03  3e-11  1e-12\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.3490e+06  2.8924e+06  1e+07  2e+02  8e+00\n",
      " 1:  8.3033e+05 -7.5156e+04  1e+06  9e+00  4e-01\n",
      " 2:  8.7752e+04  2.2658e+04  9e+04  4e-01  2e-02\n",
      " 3:  4.6128e+04  2.9194e+04  2e+04  9e-02  4e-03\n",
      " 4:  4.4531e+04  3.2998e+04  1e+04  5e-02  2e-03\n",
      " 5:  4.2440e+04  3.5362e+04  9e+03  3e-02  1e-03\n",
      " 6:  4.0857e+04  3.7107e+04  4e+03  1e-02  5e-04\n",
      " 7:  4.0378e+04  3.7649e+04  3e+03  8e-03  3e-04\n",
      " 8:  3.9674e+04  3.8384e+04  1e+03  3e-03  1e-04\n",
      " 9:  3.9397e+04  3.8673e+04  8e+02  1e-03  5e-05\n",
      "10:  3.9184e+04  3.8882e+04  3e+02  5e-04  2e-05\n",
      "11:  3.9112e+04  3.8949e+04  2e+02  1e-04  6e-06\n",
      "12:  3.9062e+04  3.8997e+04  7e+01  4e-05  2e-06\n",
      "13:  3.9032e+04  3.9026e+04  6e+00  2e-06  8e-08\n",
      "14:  3.9029e+04  3.9028e+04  2e-01  7e-08  3e-09\n",
      "15:  3.9029e+04  3.9029e+04  2e-03  7e-10  3e-11\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.4594e+08  3.1311e+08  1e+09  2e+03  9e+00\n",
      " 1:  6.3498e+07 -2.0700e+07  2e+08  2e+02  8e-01\n",
      " 2:  1.3557e+07 -2.5174e+05  2e+07  1e+01  6e-02\n",
      " 3:  7.7656e+05  2.0763e+05  9e+05  5e-01  2e-03\n",
      " 4:  5.6661e+05  2.4772e+05  4e+05  2e-01  1e-03\n",
      " 5:  4.6544e+05  2.8486e+05  2e+05  1e-01  5e-04\n",
      " 6:  4.2513e+05  3.1061e+05  1e+05  5e-02  2e-04\n",
      " 7:  3.9464e+05  3.2837e+05  8e+04  3e-02  1e-04\n",
      " 8:  3.8350e+05  3.3617e+05  6e+04  2e-02  7e-05\n",
      " 9:  3.7248e+05  3.4388e+05  3e+04  9e-03  4e-05\n",
      "10:  3.6450e+05  3.4976e+05  2e+04  4e-03  2e-05\n",
      "11:  3.6114e+05  3.5223e+05  1e+04  2e-03  9e-06\n",
      "12:  3.5885e+05  3.5389e+05  5e+03  9e-04  4e-06\n",
      "13:  3.5721e+05  3.5514e+05  2e+03  3e-04  1e-06\n",
      "14:  3.5626e+05  3.5591e+05  4e+02  4e-05  2e-07\n",
      "15:  3.5617e+05  3.5598e+05  2e+02  6e-06  3e-08\n",
      "16:  3.5614e+05  3.5601e+05  1e+02  2e-06  7e-09\n",
      "17:  3.5609e+05  3.5606e+05  3e+01  3e-07  1e-09\n",
      "18:  3.5608e+05  3.5606e+05  1e+01  5e-16  3e-13\n",
      "19:  3.5607e+05  3.5607e+05  3e+00  5e-16  4e-12\n",
      "20:  3.5607e+05  3.5607e+05  2e-01  5e-16  5e-12\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.4243e+08  2.9747e+08  1e+09  2e+03  8e+00\n",
      " 1:  6.5192e+07 -1.6077e+07  2e+08  1e+02  6e-01\n",
      " 2:  1.0603e+07 -2.0436e+03  2e+07  9e+00  4e-02\n",
      " 3:  6.8078e+05  2.1866e+05  7e+05  4e-01  2e-03\n",
      " 4:  5.8577e+05  2.6059e+05  4e+05  2e-01  1e-03\n",
      " 5:  4.6099e+05  3.0881e+05  2e+05  9e-02  4e-04\n",
      " 6:  4.3301e+05  3.3134e+05  1e+05  5e-02  2e-04\n",
      " 7:  4.0907e+05  3.4848e+05  7e+04  3e-02  1e-04\n",
      " 8:  3.9583e+05  3.5879e+05  4e+04  1e-02  6e-05\n",
      " 9:  3.8948e+05  3.6410e+05  3e+04  8e-03  3e-05\n",
      "10:  3.8272e+05  3.6916e+05  2e+04  3e-03  1e-05\n",
      "11:  3.7991e+05  3.7149e+05  9e+03  2e-03  7e-06\n",
      "12:  3.7728e+05  3.7365e+05  4e+03  6e-04  3e-06\n",
      "13:  3.7552e+05  3.7511e+05  4e+02  5e-05  2e-07\n",
      "14:  3.7534e+05  3.7526e+05  8e+01  8e-06  4e-08\n",
      "15:  3.7531e+05  3.7529e+05  2e+01  1e-06  5e-09\n",
      "16:  3.7531e+05  3.7530e+05  1e+01  4e-07  2e-09\n",
      "17:  3.7530e+05  3.7530e+05  5e-01  2e-08  9e-11\n",
      "18:  3.7530e+05  3.7530e+05  5e-03  2e-10  9e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.3268e+08  2.5451e+08  9e+08  2e+03  7e+00\n",
      " 1:  4.8594e+07 -1.1214e+07  1e+08  1e+02  5e-01\n",
      " 2:  5.7442e+06  1.6335e+05  9e+06  6e+00  2e-02\n",
      " 3:  6.2139e+05  2.3480e+05  6e+05  4e-01  1e-03\n",
      " 4:  5.7960e+05  2.6881e+05  5e+05  3e-01  1e-03\n",
      " 5:  4.8001e+05  3.1971e+05  2e+05  1e-01  4e-04\n",
      " 6:  4.5060e+05  3.4625e+05  1e+05  7e-02  2e-04\n",
      " 7:  4.2745e+05  3.6597e+05  8e+04  3e-02  1e-04\n",
      " 8:  4.2039e+05  3.7342e+05  6e+04  2e-02  8e-05\n",
      " 9:  4.1101e+05  3.8210e+05  4e+04  1e-02  4e-05\n",
      "10:  4.0587e+05  3.8692e+05  2e+04  7e-03  3e-05\n",
      "11:  4.0206e+05  3.9044e+05  1e+04  4e-03  1e-05\n",
      "12:  3.9987e+05  3.9243e+05  9e+03  2e-03  8e-06\n",
      "13:  3.9755e+05  3.9444e+05  3e+03  6e-04  2e-06\n",
      "14:  3.9637e+05  3.9552e+05  9e+02  1e-04  4e-07\n",
      "15:  3.9612e+05  3.9574e+05  4e+02  4e-05  2e-07\n",
      "16:  3.9595e+05  3.9590e+05  5e+01  2e-06  6e-09\n",
      "17:  3.9593e+05  3.9592e+05  8e+00  2e-07  9e-10\n",
      "18:  3.9593e+05  3.9592e+05  8e-02  3e-09  9e-12\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.4253e+08  2.8357e+08  1e+09  2e+03  8e+00\n",
      " 1:  6.1823e+07 -1.3785e+07  2e+08  1e+02  5e-01\n",
      " 2:  8.7275e+06  8.2678e+04  1e+07  7e+00  3e-02\n",
      " 3:  6.8704e+05  2.1856e+05  7e+05  4e-01  2e-03\n",
      " 4:  6.1148e+05  2.4687e+05  5e+05  3e-01  1e-03\n",
      " 5:  4.8376e+05  3.0187e+05  2e+05  7e-02  3e-04\n",
      " 6:  4.4520e+05  3.2445e+05  1e+05  4e-02  2e-04\n",
      " 7:  4.1470e+05  3.4165e+05  9e+04  2e-02  9e-05\n",
      " 8:  4.0050e+05  3.5019e+05  6e+04  1e-02  5e-05\n",
      " 9:  3.8591e+05  3.5982e+05  3e+04  6e-03  2e-05\n",
      "10:  3.7904e+05  3.6450e+05  2e+04  3e-03  1e-05\n",
      "11:  3.7526e+05  3.6714e+05  9e+03  1e-03  5e-06\n",
      "12:  3.7256e+05  3.6912e+05  4e+03  4e-04  2e-06\n",
      "13:  3.7161e+05  3.6980e+05  2e+03  2e-04  7e-07\n",
      "14:  3.7111e+05  3.7019e+05  1e+03  6e-05  2e-07\n",
      "15:  3.7083e+05  3.7042e+05  4e+02  1e-05  6e-08\n",
      "16:  3.7063e+05  3.7061e+05  3e+01  8e-07  3e-09\n",
      "17:  3.7062e+05  3.7062e+05  6e-01  2e-08  7e-11\n",
      "18:  3.7062e+05  3.7062e+05  6e-03  2e-10  8e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.3704e+08  2.7762e+08  1e+09  2e+03  7e+00\n",
      " 1:  7.7131e+07 -8.7869e+06  1e+08  9e+01  3e-01\n",
      " 2:  4.8777e+06  2.0288e+05  6e+06  2e+00  1e-02\n",
      " 3:  6.2584e+05  2.3651e+05  5e+05  2e-01  8e-04\n",
      " 4:  5.2563e+05  2.9900e+05  3e+05  9e-02  4e-04\n",
      " 5:  4.6812e+05  3.3309e+05  2e+05  5e-02  2e-04\n",
      " 6:  4.3820e+05  3.5352e+05  1e+05  3e-02  1e-04\n",
      " 7:  4.1848e+05  3.6797e+05  6e+04  1e-02  6e-05\n",
      " 8:  4.0700e+05  3.7692e+05  3e+04  8e-03  3e-05\n",
      " 9:  4.0058e+05  3.8194e+05  2e+04  4e-03  1e-05\n",
      "10:  3.9546e+05  3.8602e+05  1e+04  2e-03  6e-06\n",
      "11:  3.9278e+05  3.8822e+05  5e+03  6e-04  2e-06\n",
      "12:  3.9158e+05  3.8918e+05  3e+03  3e-04  1e-06\n",
      "13:  3.9084e+05  3.8978e+05  1e+03  1e-04  4e-07\n",
      "14:  3.9048e+05  3.9008e+05  4e+02  2e-05  8e-08\n",
      "15:  3.9031e+05  3.9024e+05  7e+01  2e-06  7e-09\n",
      "16:  3.9027e+05  3.9027e+05  7e-01  2e-08  7e-11\n",
      "17:  3.9027e+05  3.9027e+05  7e-03  2e-10  7e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.8180e+02  2.5689e+03  1e+04  4e+00  5e+01\n",
      " 1:  1.4246e+03 -3.5195e+02  2e+03  5e-01  6e+00\n",
      " 2:  7.1627e+02  3.0585e+02  5e+02  9e-02  1e+00\n",
      " 3:  5.1787e+02  4.2812e+02  1e+02  2e-02  2e-01\n",
      " 4:  4.9480e+02  4.5498e+02  4e+01  6e-03  7e-02\n",
      " 5:  4.8677e+02  4.6408e+02  2e+01  3e-03  3e-02\n",
      " 6:  4.8043e+02  4.7045e+02  1e+01  1e-03  1e-02\n",
      " 7:  4.7714e+02  4.7374e+02  4e+00  3e-04  4e-03\n",
      " 8:  4.7566e+02  4.7522e+02  5e-01  2e-05  2e-04\n",
      " 9:  4.7548e+02  4.7539e+02  1e-01  4e-06  5e-05\n",
      "10:  4.7545e+02  4.7543e+02  2e-02  6e-07  7e-06\n",
      "11:  4.7544e+02  4.7544e+02  2e-04  7e-09  8e-08\n",
      "Optimal solution found.\n",
      "Best C: 1.0\n",
      "Best Cross-Validation Test Accuracy: 0.8678\n",
      "Best Cross-Validation Train Accuracy: 0.7955\n",
      "Cross-Validation Run Time: 12.5849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "def cross_validate_svm(X, y, C_values, n_splits=5):\n",
    "    \"\"\"\n",
    "    Perform 5-fold cross-validation to tune the hyperparameter C.\n",
    "\n",
    "    Args:\n",
    "        X: Input data (N x d)\n",
    "        y: Labels (N x 1)\n",
    "        C_values: List of regularization parameters C to test\n",
    "        n_splits: Number of folds for cross-validation (default is 5)\n",
    "\n",
    "    Returns:\n",
    "        best_C: The best regularization parameter C based on cross-validation\n",
    "        best_accuracy: The corresponding average accuracy\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=44)\n",
    "    best_C = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "\n",
    "    # Perform cross-validation for each C value\n",
    "    for C in C_values:\n",
    "        accuracies = []\n",
    "        train_accuracies = []\n",
    "\n",
    "        for train_idx, val_idx in kf.split(X):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "            # Standardize the features using the training data\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_val_scaled = scaler.transform(X_val)  # Apply the same scaling to the validation set\n",
    "\n",
    "            # Train the SVM with the current C (replace this with your linear soft margin SVM)\n",
    "            w, b = linear_soft_margin_svm(X_train_scaled, y_train, C)\n",
    "\n",
    "            # Predict and evaluate on validation set\n",
    "            y_pred_val = predict(X_val_scaled, w, b)\n",
    "            y_pred_train = predict(X_train_scaled, w, b)\n",
    "            accuracy = np.mean(y_pred_val == y_val)\n",
    "            train_accuracy = np.mean(y_pred_train == y_train)\n",
    "            accuracies.append(accuracy)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Calculate the average accuracy for the current C\n",
    "        avg_accuracy = np.mean(accuracies)\n",
    "        avg_train_accuracy = np.mean(train_accuracies)\n",
    "\n",
    "        # Update the best C if necessary\n",
    "        if avg_accuracy > best_accuracy:\n",
    "            best_accuracy = avg_accuracy\n",
    "            best_train_accuracy = avg_train_accuracy\n",
    "            best_C = C\n",
    "\n",
    "\n",
    "    return best_C, best_accuracy, best_train_accuracy\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    X = pd.read_excel(\"../coffeeDataSynthesized.xlsx\", \"dataset\")\n",
    "    y = np.where(X[\"type\"] == \"robusta\", 0, 1)  # Robust -> 0, Arabica -> 1\n",
    "    y = pd.Series(y)\n",
    "\n",
    "    # Select relevant features\n",
    "    X = X[['width', 'height', 'depth', 'weight']]\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)\n",
    "\n",
    "    # Scale the features (using StandardScaler for consistency across all steps)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)  # Apply the same scaling to the test data\n",
    "\n",
    "    # Convert y_train and y_test to -1 and +1 (SVM convention)\n",
    "    y_train = np.where(y_train == 0, -1, 1)\n",
    "    y_test = np.where(y_test == 0, -1, 1)\n",
    "\n",
    "    # Set a range of C values to test\n",
    "    C_values = [0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    # Tune hyperparameter C using cross-validation\n",
    "    best_C, best_accuracy, best_train_accuracy = cross_validate_svm(X_train_scaled, y_train, C_values)\n",
    "    w, b = linear_soft_margin_svm(X_train_scaled, y_train, best_C)\n",
    "    end_time = time.time()\n",
    "    y_pred_custom_cv = predict(X_test_scaled, w, b)\n",
    "    y_train_pred_custom_cv = predict(X_train_scaled, w, b)\n",
    "    custom_accuracy_cv = np.mean(y_pred_custom_cv == y_test)\n",
    "    custom_accuracy_train_cv = np.mean(y_train_pred_custom_cv == y_train)\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    print(f\"Best C: {best_C}\")\n",
    "    print(f\"Best Cross-Validation Test Accuracy: {custom_accuracy_cv:.4f}\")\n",
    "    print(f\"Best Cross-Validation Train Accuracy: {custom_accuracy_train_cv:.4f}\")\n",
    "    print(f\"Cross-Validation Run Time: {training_time:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different scikit learn libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM Accuracy: 0.8678, Train Accuracy: 0.7955, Time: 0.0453 seconds\n",
      "RBF SVM Accuracy: 0.8760, Train Accuracy: 0.8771, Time: 0.0469 seconds\n",
      "Polynomial SVM (degree=3) Test Accuracy: 0.7603, Train Accuracy: 0.7727 , Time: 0.0232 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time  # Import time module\n",
    "\n",
    "# Train and evaluate Linear SVM\n",
    "start_time = time.time()  # Start time\n",
    "linear_svm = SVC(kernel='linear')\n",
    "linear_svm.fit(X_train_scaled, y_train)\n",
    "y_pred_linear = linear_svm.predict(X_test_scaled)\n",
    "y_pred_train_linear = linear_svm.predict(X_train_scaled)\n",
    "linear_accuracy = accuracy_score(y_test, y_pred_linear)\n",
    "linear_train_accuracy = accuracy_score(y_train, y_pred_train_linear)\n",
    "linear_time = time.time() - start_time  # End time and calculate duration\n",
    "print(f\"Linear SVM Accuracy: {linear_accuracy:.4f}, Train Accuracy: {linear_train_accuracy:.4f}, Time: {linear_time:.4f} seconds\")\n",
    "\n",
    "# Train and evaluate RBF SVM\n",
    "start_time = time.time()  # Start time\n",
    "rbf_svm = SVC(kernel='rbf')\n",
    "rbf_svm.fit(X_train_scaled, y_train)\n",
    "y_pred_rbf = rbf_svm.predict(X_test_scaled)\n",
    "y_pred_train_rbf = rbf_svm.predict(X_train_scaled)\n",
    "rbf_accuracy = accuracy_score(y_test, y_pred_rbf)\n",
    "rbf_train_accuracy = accuracy_score(y_train, y_pred_train_rbf)\n",
    "rbf_time = time.time() - start_time  # End time and calculate duration\n",
    "print(f\"RBF SVM Accuracy: {rbf_accuracy:.4f}, Train Accuracy: {rbf_train_accuracy:.4f}, Time: {rbf_time:.4f} seconds\")\n",
    "\n",
    "# Train and evaluate Polynomial SVM (degree=3)\n",
    "start_time = time.time()  # Start time\n",
    "poly_svm = SVC(kernel='poly', degree=3)\n",
    "poly_svm.fit(X_train_scaled, y_train)\n",
    "y_pred_poly = poly_svm.predict(X_test_scaled)\n",
    "y_pred_train_poly = poly_svm.predict(X_train_scaled)\n",
    "poly_accuracy = accuracy_score(y_test, y_pred_poly)\n",
    "poly_train_accuracy = accuracy_score(y_train, y_pred_train_poly)\n",
    "poly_time = time.time() - start_time  # End time and calculate duration\n",
    "print(f\"Polynomial SVM (degree=3) Test Accuracy: {poly_accuracy:.4f}, Train Accuracy: {poly_train_accuracy:.4f} , Time: {poly_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[102  18]\n",
      " [ 14 108]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred_linear)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[101  19]\n",
      " [ 11 111]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred_rbf)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 64  56]\n",
      " [  2 120]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred_poly)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Accuracy: 0.867769\n",
      "Our Recall: 0.901639\n",
      "Our Precision: 0.846154\n",
      "Our F1-score: 0.873016\n",
      "Our AUROC: 0.867486\n",
      "\n",
      "CV Accuracy: 0.867769\n",
      "CV Recall: 0.885246\n",
      "CV Precision: 0.857143\n",
      "CV F1-score: 0.870968\n",
      "CV AUROC: 0.867623\n",
      "\n",
      "Linear Accuracy: 0.867769\n",
      "Linear Recall: 0.885246\n",
      "Linear Precision: 0.857143\n",
      "Linear F1-score: 0.870968\n",
      "Linear AUROC: 0.867623\n",
      "\n",
      "RBF Accuracy: 0.876033\n",
      "RBF Recall: 0.909836\n",
      "RBF Precision: 0.853846\n",
      "RBF F1-score: 0.880952\n",
      "RBF AUROC: 0.875751\n",
      "\n",
      "Polynomial Accuracy: 0.760331\n",
      "Polynomial Recall: 0.983607\n",
      "Polynomial Precision: 0.681818\n",
      "Polynomial F1-score: 0.805369\n",
      "Polynomial AUROC: 0.758470\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# Example: Recall, Precision, and F1-score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_custom)\n",
    "recall = recall_score(y_test, y_pred_custom)\n",
    "precision = precision_score(y_test, y_pred_custom)\n",
    "f1 = f1_score(y_test, y_pred_custom)\n",
    "auroc = roc_auc_score(y_test, y_pred_custom)\n",
    "\n",
    "cv_accuracy = accuracy_score(y_test, y_pred_custom_cv)\n",
    "cv_recall = recall_score(y_test, y_pred_custom_cv)\n",
    "cv_precision = precision_score(y_test, y_pred_custom_cv)\n",
    "cv_f1 = f1_score(y_test, y_pred_custom_cv)\n",
    "cv_auroc = roc_auc_score(y_test, y_pred_custom_cv)\n",
    "\n",
    "linear_accuracy = accuracy_score(y_test, y_pred_linear)\n",
    "linear_recall = recall_score(y_test, y_pred_linear)\n",
    "linear_precision = precision_score(y_test, y_pred_linear)\n",
    "linear_f1 = f1_score(y_test, y_pred_linear)\n",
    "linear_auroc = roc_auc_score(y_test, y_pred_linear)\n",
    "\n",
    "rbf_accuracy = accuracy_score(y_test, y_pred_rbf)\n",
    "rbf_recall = recall_score(y_test, y_pred_rbf)\n",
    "rbf_precision = precision_score(y_test, y_pred_rbf)\n",
    "rbf_f1 = f1_score(y_test, y_pred_rbf)\n",
    "rbf_auroc = roc_auc_score(y_test, y_pred_rbf)\n",
    "\n",
    "poly_accuracy = accuracy_score(y_test, y_pred_poly)\n",
    "poly_recall = recall_score(y_test, y_pred_poly)\n",
    "poly_precision = precision_score(y_test, y_pred_poly)\n",
    "poly_f1 = f1_score(y_test, y_pred_poly)\n",
    "poly_auroc = roc_auc_score(y_test, y_pred_poly)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print all metrics\n",
    "print(f\"Our Accuracy: {accuracy:4f}\")\n",
    "print(f\"Our Recall: {recall:4f}\")\n",
    "print(f\"Our Precision: {precision:4f}\")\n",
    "print(f\"Our F1-score: {f1:4f}\"),\n",
    "print(f\"Our AUROC: {auroc:4f}\")\n",
    "print()\n",
    "print(f\"CV Accuracy: {cv_accuracy:4f}\")\n",
    "print(f\"CV Recall: {cv_recall:4f}\")\n",
    "print(f\"CV Precision: {cv_precision:4f}\")\n",
    "print(f\"CV F1-score: {cv_f1:4f}\"),\n",
    "print(f\"CV AUROC: {cv_auroc:4f}\")\n",
    "print()\n",
    "print(f\"Linear Accuracy: {linear_accuracy:4f}\")\n",
    "print(f\"Linear Recall: {linear_recall:4f}\")\n",
    "print(f\"Linear Precision: {linear_precision:4f}\")\n",
    "print(f\"Linear F1-score: {linear_f1:4f}\"),\n",
    "print(f\"Linear AUROC: {linear_auroc:4f}\")\n",
    "print()\n",
    "print(f\"RBF Accuracy: {rbf_accuracy:4f}\")\n",
    "print(f\"RBF Recall: {rbf_recall:4f}\")\n",
    "print(f\"RBF Precision: {rbf_precision:4f}\")\n",
    "print(f\"RBF F1-score: {rbf_f1:4f}\")\n",
    "print(f\"RBF AUROC: {rbf_auroc:4f}\")\n",
    "print()\n",
    "print(f\"Polynomial Accuracy: {poly_accuracy:4f}\")\n",
    "print(f\"Polynomial Recall: {poly_recall:4f}\")\n",
    "print(f\"Polynomial Precision: {poly_precision:4f}\")\n",
    "print(f\"Polynomial F1-score: {poly_f1:4f}\")\n",
    "print(f\"Polynomial AUROC: {poly_auroc:4f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
