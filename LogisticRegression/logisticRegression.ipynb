{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.692\n",
      "Iteration 500, Loss: 0.635\n",
      "Iteration 1000, Loss: 0.612\n",
      "Iteration 1500, Loss: 0.598\n",
      "Iteration 2000, Loss: 0.589\n",
      "Iteration 2500, Loss: 0.583\n",
      "Iteration 3000, Loss: 0.578\n",
      "Iteration 3500, Loss: 0.575\n",
      "Iteration 4000, Loss: 0.571\n",
      "Converged after 4371 iterations.\n",
      "Logistic Regression using Scikit-learn\n",
      "Train Accuracy: 0.7634\n",
      "Test Accuracy: 0.8388\n",
      "Runtime: 0.0128 seconds\n",
      "Logistic Regression from Scratch\n",
      "Train Accuracy: 0.6488\n",
      "Test Accuracy: 0.6529\n",
      "Runtime: 4.0333 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Logistic Regression from Scratch\n",
    "class LogisticRegressionScratch:\n",
    "    def __init__(self, learning_rate=0.05, max_iter=5000, regularization=None, lambda_=0.01, tol=1e-2):  # Fixed _init_ method\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.regularization = regularization\n",
    "        self.lambda_ = lambda_  # Changed lambda to lambda_\n",
    "        self.tol = tol  # Stopping tolerance for gradient magnitude\n",
    "        self.theta = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def compute_loss(self, X, y):\n",
    "        m = len(y)\n",
    "        h = self.sigmoid(X @ self.theta)\n",
    "        loss = -(1/m) * (y @ np.log(h) + (1 - y) @ np.log(1 - h))\n",
    "        if self.regularization == 'l2':\n",
    "            loss += (self.lambda_ / (2 * m)) * np.sum(np.square(self.theta[1:]))\n",
    "        return loss\n",
    "\n",
    "    def gradient(self, X, y):\n",
    "        m = len(y)\n",
    "        h = self.sigmoid(X @ self.theta)\n",
    "        gradient = (1/m) * (X.T @ (h - y))\n",
    "        if self.regularization == 'l2':\n",
    "            gradient[1:] += (self.lambda_ / m) * self.theta[1:]\n",
    "        return gradient\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.theta = np.zeros(n)\n",
    "        for i in range(self.max_iter):\n",
    "            grad = self.gradient(X, y)\n",
    "            grad_magnitude = np.linalg.norm(grad)\n",
    "            if grad_magnitude < self.tol:  # Stop if gradient is small enough\n",
    "                print(f\"Converged after {i+1} iterations.\")\n",
    "                break\n",
    "            self.theta -= self.learning_rate * grad\n",
    "            \n",
    "            if i % 500 == 0:  # Print loss value every 100 iterations\n",
    "                print(f\"Iteration {i}, Loss: {round(self.compute_loss(X, y),3)}\")\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(X @ self.theta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "\n",
    "X = pd.read_excel(\"../coffeeDataSynthesized.xlsx\", \"dataset\")\n",
    "y = np.where(X[\"type\"] == \"robusta\", 0, 1)\n",
    "y = pd.Series(y)\n",
    "\n",
    "X = X[['width', 'height', 'depth', 'weight']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)\n",
    "\n",
    "# Train Logistic Regression from Scratch\n",
    "model_scratch = LogisticRegressionScratch(learning_rate=0.05, max_iter=5000,  lambda_=0.1, tol=1e-2)\n",
    "start_time = time.time()\n",
    "model_scratch.fit(X_train, y_train)\n",
    "scratch_time = time.time() - start_time\n",
    "\n",
    "# Predictions and accuracy\n",
    "y_train_pred_scratch = model_scratch.predict(X_train)\n",
    "y_test_pred_scratch = model_scratch.predict(X_test)\n",
    "\n",
    "train_accuracy_scratch = accuracy_score(y_train, y_train_pred_scratch)\n",
    "test_accuracy_scratch = accuracy_score(y_test, y_test_pred_scratch)\n",
    "\n",
    "# Measure training time for Scikit-learn Logistic Regression\n",
    "start_time = time.time()\n",
    "model_sklearn = LogisticRegression( C=10, max_iter=5000, solver='lbfgs', tol=1e-2)\n",
    "model_sklearn.fit(X_train, y_train)  # Exclude intercept term\n",
    "sklearn_time = time.time() - start_time\n",
    "\n",
    "# Predictions and accuracy\n",
    "y_train_pred_sklearn = model_sklearn.predict(X_train)\n",
    "y_test_pred_sklearn = model_sklearn.predict(X_test)\n",
    "\n",
    "train_accuracy_sklearn = accuracy_score(y_train, y_train_pred_sklearn)\n",
    "test_accuracy_sklearn = accuracy_score(y_test, y_test_pred_sklearn)\n",
    "# Report Results\n",
    "\n",
    "\n",
    "print(\"Logistic Regression using Scikit-learn\")\n",
    "print(f\"Train Accuracy: {train_accuracy_sklearn:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_sklearn:.4f}\")\n",
    "print(f\"Runtime: {sklearn_time:.4f} seconds\")\n",
    "\n",
    "print(\"Logistic Regression from Scratch\")\n",
    "print(f\"Train Accuracy: {train_accuracy_scratch:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_scratch:.4f}\")\n",
    "print(f\"Runtime: {scratch_time:.4f} seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 48  72]\n",
      " [ 12 110]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred_scratch)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 91  29]\n",
      " [ 10 112]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred_sklearn)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Accuracy: 0.6529\n",
      "Our Recall: 0.9016\n",
      "Our Precision: 0.6044\n",
      "Our F1-score: 0.7237\n",
      "Our AUROC: 0.6508\n",
      "\n",
      "Sklearn Accuracy: 0.8388\n",
      "Sklearn Recall: 0.9180\n",
      "Sklearn Precision: 0.7943\n",
      "Sklearn F1-score: 0.8517\n",
      "Sklearn AUROC: 0.8382\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# Assuming y_true and y_pred are our true labels and predicted labels\n",
    "# For probabilities, use y_pred_proba for AUROC and average precision.\n",
    "model_scratch = LogisticRegressionScratch(learning_rate=0.05, max_iter=5000, regularization='l2', lambda_=0.1, tol=1e-2)\n",
    "\n",
    "# Example: Classification accuracy\n",
    "sk_accuracy = accuracy_score(y_test, y_test_pred_sklearn)\n",
    "# Example: Recall, Precision, and F1-score\n",
    "sk_recall = recall_score(y_test, y_test_pred_sklearn)\n",
    "sk_precision = precision_score(y_test, y_test_pred_sklearn)\n",
    "sk_f1 = f1_score(y_test, y_test_pred_sklearn)\n",
    "sk_auroc = roc_auc_score(y_test, y_test_pred_sklearn)\n",
    "\n",
    "\n",
    "\n",
    "sc_accuracy = accuracy_score(y_test, y_test_pred_scratch)\n",
    "# Example: Recall, Precision, and F1-score\n",
    "sc_recall = recall_score(y_test, y_test_pred_scratch)\n",
    "sc_precision = precision_score(y_test, y_test_pred_scratch)\n",
    "sc_f1 = f1_score(y_test, y_test_pred_scratch)\n",
    "sc_auroc = roc_auc_score(y_test, y_test_pred_scratch)\n",
    "\n",
    "# Print all metrics\n",
    "print(f\"Our Accuracy: {sc_accuracy:.4f}\")\n",
    "print(f\"Our Recall: {sc_recall:.4f}\")\n",
    "print(f\"Our Precision: {sc_precision:.4f}\")\n",
    "print(f\"Our F1-score: {sc_f1:.4f}\"),\n",
    "print(f\"Our AUROC: {sc_auroc:.4f}\")\n",
    "print()\n",
    "print(f\"Sklearn Accuracy: {sk_accuracy:.4f}\")\n",
    "print(f\"Sklearn Recall: {sk_recall:.4f}\")\n",
    "print(f\"Sklearn Precision: {sk_precision:.4f}\")\n",
    "print(f\"Sklearn F1-score: {sk_f1:.4f}\"),\n",
    "print(f\"Sklearn AUROC: {sk_auroc:.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
